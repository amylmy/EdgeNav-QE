# QLoRA core configuration (extends LoRA)
peft_config:
  peft_type: LORA
  r: 8
  lora_alpha: 16
  lora_dropout: 0.05
  bias: "none"
  task_type: "VISION_AND_LANGUAGE_GENERATION"
  target_modules: ["q_proj", "v_proj"]

# Quantization config (critical for QLoRA)
quantization_config:
  load_in_4bit: true  # Use 4-bit quantization (set to 8bit for 8-bit)
  bnb_4bit_use_double_quant: true  # Double quantization (extra memory savings)
  bnb_4bit_quant_type: "nf4"  # Normalized float4 (optimal for LLMs)
  bnb_4bit_compute_dtype: torch.float16  # Compute in FP16 (balance speed/precision)

# Training configuration (optimized for QLoRA)
training_config:
  model_name_or_path: "../models/openvla-7b"
  train_data_path: "./data/demo_push_dataset/train.jsonl"
  val_data_path: "./data/demo_push_dataset/val.jsonl"
  output_dir: "./finetuned_openvla_qlora"
  per_device_train_batch_size: 4  # Larger batch size (thanks to QLoRA)
  gradient_accumulation_steps: 2
  learning_rate: 5e-5  # Same as LoRA (no need to adjust)
  num_train_epochs: 10
  logging_steps: 10
  save_steps: 50
  fp16: true
  attn_implementation: "eager"  # Mandatory for macOS
  device: "mps"  # "cuda" for Linux GPUs, "cpu" as fallback